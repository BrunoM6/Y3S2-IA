{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d04c6591",
   "metadata": {},
   "source": [
    "# Streaming Videos Cache Optimization Problem\n",
    "Using cache servers, we can optimize requests for videos from a data center to endpoints. Based on the predicted requests from endpoints, can we find a way to optimize the distribution and storage of said videos?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4340f43",
   "metadata": {},
   "source": [
    "## Input and Parsing\n",
    "Data is provided as text. We can parse said data into various tokens. We read from `data/input.txt` in this case.\n",
    "The `problem_description` array holds, in order, from 0 to 4, the number of videos, number of endpoints, number of request descriptions, number of cache servers, and the capacity of each cache server in megabytes.\n",
    "The `video_size` array holds the size of each video in MB.\n",
    "We then parse based on the ammount of endpoints, to connect each endpoint to the caches. The `endpoint_data_description` describes the latency between an endpoint (serves as the index of the array) and the data center (latency is the value stored), and the `endpoint_cache_description` has the key/value specification of key:(endpoint, cache) -> value:latency.\n",
    "Finally, `request_description` is a dictionary that holds the ammount of requests a certain video at an endpoint holds, specification of key:(endpoint, video) -> value:nº of requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41de98bf-39dd-47ee-ad0a-84030113bc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_results(file: str):\n",
    "    problem_description = []\n",
    "    video_size = []\n",
    "    endpoint_data_description = []\n",
    "    endpoint_cache_description = {}\n",
    "    request_description = {}\n",
    "    with open('data/' + file, 'r') as file:\n",
    "        line = file.readline()\n",
    "        tokens = line.strip().split()\n",
    "        for token in tokens:\n",
    "            problem_description.append(int(token))\n",
    "        line = file.readline()\n",
    "        tokens = line.strip().split()\n",
    "        for token in tokens:\n",
    "            video_size.append(int(token))\n",
    "        i = 0\n",
    "        while i != problem_description[1]:\n",
    "            line = file.readline()\n",
    "            tokens = line.strip().split()\n",
    "            endpoint_data_description.append(int(tokens[0]))\n",
    "            connections = int(tokens[1])\n",
    "            j = 0\n",
    "            while j < connections:\n",
    "                line = file.readline()\n",
    "                tokens = line.strip().split()\n",
    "                endpoint_cache_description[(i, tokens[0])] = tokens[1]\n",
    "                j += 1\n",
    "            i += 1\n",
    "        i = 0\n",
    "        c = 0\n",
    "        while i != problem_description[2]:\n",
    "            i+=1\n",
    "            line = file.readline()\n",
    "            tokens = line.strip().split()\n",
    "            key = (tokens[1], tokens[0])\n",
    "            if key in request_description:\n",
    "                request_description[key] += tokens[2]\n",
    "            else:\n",
    "                request_description[key] = tokens[2]\n",
    "    return problem_description, video_size, endpoint_data_description, endpoint_cache_description, request_description\n",
    "\n",
    "problem_description, video_size, endpoint_data_description, endpoint_cache_description, request_description = parse_results('me_at_the_zoo.in')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a51df3c",
   "metadata": {},
   "source": [
    "## Problem State\n",
    "The problem state is defined by a dictionary that maps a cache to a list of videos. We must careful with the underlying constraints of the total video sizes not surpassing cache size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93dbfa4",
   "metadata": {},
   "source": [
    "## Goal and Scoring\n",
    "Our goal is to maximize time saved by the caches, for this, we must go through our current cache configuration and figure out how much time we are saving in total based on the requests. Then, we multiply this value, in milliseconds, by 1000, to get the score. \n",
    "Time Saved (Request Description) = Nº of Requests * min(Latency of Data Center - Latency of Cache with Video)\n",
    "Also, when re-scoring our problem, it makes more sense to update the current score with the alteration instead of re-calculating the score from scratch.\n",
    "The score presumes a valid problem state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3234972a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def score(problem_state: dict, endpoint_data_description: list, endpoint_cache_description: dict, request_description: dict) -> float:\n",
    "    score = 0\n",
    "    for (endpoint, video), request_number in request_description:\n",
    "        data_center_latency = endpoint_data_description[endpoint]\n",
    "        cache_latency = data_center_latency\n",
    "        for cache, videos in problem_state:\n",
    "            if video in videos:\n",
    "                cache_latency = min(cache_latency, endpoint_cache_description[(endpoint, cache)])\n",
    "        score += (data_center_latency - cache_latency) * request_number\n",
    "    return math.floor(score * 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f6a01b",
   "metadata": {},
   "source": [
    "### Updating the score\n",
    "Now, for computational efficiency effects, we create a re-score function that based on a cache change, a current score and the descriptions, updates the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdafcc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_score(problem_state):\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5ce750",
   "metadata": {},
   "source": [
    "## Greedy Algorithm\n",
    "\n",
    "In order to have an initial solution to which we can apply the several heuristics to, we will develop a **greedy algorithm** to calculate a **potential solution state**. Our goal is to use this as a **starting point** that already has some value and logic and that will hopefully lead us to a **nearly ideal end result**.\n",
    "\n",
    "For our algorithm we need a way to **evaluate the quality or benefit** of a specific solution. With this intent in mind, we will compute a **simplified score** for each cache/video pair, that corresponds to the saved latency.\n",
    "\n",
    "(DC latency - Cache latency) * no. of requests\n",
    "\n",
    " After saving the score for the pairs we will **sort them** and then treat this as a sort of **knapsack problem** - put them in caches in order of most to least time saved until all cache/video pairs are iterated over or until all caches are full.\n",
    "\n",
    " The result of this method will be a dictionary data structure where the keys are cache IDs and the values are the videos stored in them (as described in the problem state above)\n",
    "\n",
    " We also implemented two helper functions:\n",
    "\n",
    " ### get_saved_time(element, video)\n",
    "\n",
    " Calculates how much time is saved on requests, at a specific endpoint, if a specific video is stored on x cache.\n",
    "\n",
    " This value is then incremented to the score associated to the (cache, video) key if video was requested previously on another endpoint connected to cache or a new key,value pair is introduced in the dictionary.\n",
    "\n",
    " ### current_cap(cache, solution)\n",
    "\n",
    " Calculates the current occupied capacity of a certain cache given a problem state.\n",
    "\n",
    " This is helpful when trying to figure out if a video can still be cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d67cc8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solution:\n",
      "\n",
      "{0: ['0', '8', '1', '65', '99', '5', '16', '10'], 1: ['0', '1', '13', '10', '7', '99', '16'], 2: ['0', '8', '4', '1', '16', '65', '99'], 3: ['0', '4', '2', '16', '5', '27', '10'], 4: ['0', '1', '13', '7', '10', '81', '16'], 5: ['0', '4', '2', '1', '10', '16', '81', '5'], 6: ['0', '4', '2', '1', '16', '5', '10'], 7: ['0', '1', '2', '4', '74', '16'], 8: ['0', '8', '1', '3', '16'], 9: ['0', '4', '8', '16', '1', '5', '82', '10']}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: ['0', '8', '1', '65', '99', '5', '16', '10'],\n",
       " 1: ['0', '1', '13', '10', '7', '99', '16'],\n",
       " 2: ['0', '8', '4', '1', '16', '65', '99'],\n",
       " 3: ['0', '4', '2', '16', '5', '27', '10'],\n",
       " 4: ['0', '1', '13', '7', '10', '81', '16'],\n",
       " 5: ['0', '4', '2', '1', '10', '16', '81', '5'],\n",
       " 6: ['0', '4', '2', '1', '16', '5', '10'],\n",
       " 7: ['0', '1', '2', '4', '74', '16'],\n",
       " 8: ['0', '8', '1', '3', '16'],\n",
       " 9: ['0', '4', '8', '16', '1', '5', '82', '10']}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def greedy_start():\n",
    "    # empty solution dictionary\n",
    "    solution = {cache: [] for cache in range(problem_description[3])}\n",
    "    cache_cap = problem_description[4]\n",
    "    scores = {}\n",
    "\n",
    "    # calculate scores \n",
    "    for (endpoint, video) in request_description.keys():\n",
    "        for element in endpoint_cache_description.keys():\n",
    "            if int(element[0]) == int(endpoint):\n",
    "                cache = element[1]\n",
    "                saved_time = get_saved_time(element, video)\n",
    "                #if exists\n",
    "                if (cache, video) in scores:\n",
    "                    scores[(cache, video)] += saved_time\n",
    "                #if not\n",
    "                else:\n",
    "                    scores[(cache, video)] = saved_time\n",
    "    \n",
    "    scores = [(cache, video, saved_time) for (cache, video), saved_time in scores.items()]\n",
    "\n",
    "\n",
    "    # sort (video, cache, cost) by cost (desc)\n",
    "    scores.sort(key=lambda a: a[2], reverse=True)\n",
    "\n",
    "    # iterate through scores and fill caches\n",
    "    for (cache, video, sc) in scores:\n",
    "        curr_cap = current_cap(cache, solution)\n",
    "        if (curr_cap < cache_cap) and (video_size[int(video)] + curr_cap <= cache_cap) and (video not in solution[int(cache)]):\n",
    "            solution[int(cache)].append(video)\n",
    "\n",
    "    print(\"solution:\\n\")\n",
    "    print(solution)\n",
    "   \n",
    "    # return greedy solution\n",
    "    return solution\n",
    "\n",
    "def get_saved_time(element, video):\n",
    "    (endpoint, cache) = element\n",
    "\n",
    "    data_center_latency = endpoint_data_description[endpoint]\n",
    "    cache_latency = endpoint_cache_description[element]\n",
    "\n",
    "    request_number = request_description.get((f\"{endpoint}\", f\"{video}\"), 0)\n",
    "\n",
    "    return (int(data_center_latency) - int(cache_latency)) * int(request_number)\n",
    "\n",
    "def current_cap(cache, solution):\n",
    "    return sum(video_size[int(v)] for v in solution[int(cache)])\n",
    "\n",
    "greedy_start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
