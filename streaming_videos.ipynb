{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d04c6591",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Streaming Videos Cache Optimization Problem\n",
    "Using cache servers, we can optimize requests for videos from a data center to endpoints. Based on the predicted requests from endpoints, can we find a way to optimize the distribution and storage of said videos?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4340f43",
   "metadata": {},
   "source": [
    "## Input and Parsing\n",
    "Data is provided as text. We can parse said data into various tokens. We read from `data/input.txt` in this case.\n",
    "The `problem_description` array holds, in order, from 0 to 4, the number of videos, number of endpoints, number of request descriptions, number of cache servers, and the capacity of each cache server in megabytes.\n",
    "The `video_size` array holds the size of each video in MB.\n",
    "We then parse based on the ammount of endpoints, to connect each endpoint to the caches. The `endpoint_data_description` describes the latency between an endpoint (serves as the index of the array) and the data center (latency is the value stored), and the `endpoint_cache_description` has the key/value specification of key:(endpoint, cache) -> value:latency.\n",
    "Finally, `request_description` is a dictionary that holds the ammount of requests a certain video at an endpoint holds, specification of key:(endpoint, video) -> value:nº of requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41de98bf-39dd-47ee-ad0a-84030113bc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_results(file: str):\n",
    "    problem_description = []\n",
    "    video_size = []\n",
    "    endpoint_data_description = []\n",
    "    endpoint_cache_description = {}\n",
    "    request_description = {}\n",
    "    with open('data/' + file, 'r') as file:\n",
    "        line = file.readline()\n",
    "        tokens = line.strip().split()\n",
    "        for token in tokens:\n",
    "            problem_description.append(int(token))\n",
    "        line = file.readline()\n",
    "        tokens = line.strip().split()\n",
    "        for token in tokens:\n",
    "            video_size.append(int(token))\n",
    "        i = 0\n",
    "        while i != problem_description[1]:\n",
    "            line = file.readline()\n",
    "            tokens = line.strip().split()\n",
    "            endpoint_data_description.append(int(tokens[0]))\n",
    "            connections = int(tokens[1])\n",
    "            j = 0\n",
    "            while j < connections:\n",
    "                line = file.readline()\n",
    "                tokens = line.strip().split()\n",
    "                endpoint_cache_description[(i, tokens[0])] = tokens[1]\n",
    "                j += 1\n",
    "            i += 1\n",
    "        i = 0\n",
    "        c = 0\n",
    "        while i != problem_description[2]:\n",
    "            i+=1\n",
    "            line = file.readline()\n",
    "            tokens = line.strip().split()\n",
    "            key = (tokens[1], tokens[0])\n",
    "            if key in request_description:\n",
    "                request_description[key] += tokens[2]\n",
    "            else:\n",
    "                request_description[key] = token[2]\n",
    "    return problem_description, video_size, endpoint_data_description, endpoint_cache_description, request_description\n",
    "\n",
    "problem_description, video_size, endpoint_data_description, endpoint_cache_description, request_description = parse_results('kittens.in.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a51df3c",
   "metadata": {},
   "source": [
    "## Problem State\n",
    "The problem state is defined by a dictionary that maps a cache to a list of videos. We must careful with the underlying constraints of the total video sizes not surpassing cache size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93dbfa4",
   "metadata": {},
   "source": [
    "## Goal and Scoring\n",
    "Our goal is to maximize time saved by the caches, for this, we must go through our current cache configuration and figure out how much time we are saving in total based on the requests. Then, we multiply this value, in milliseconds, by 1000, to get the score. \n",
    "Time Saved (Request Description) = Nº of Requests * min(Latency of Data Center - Latency of Cache with Video)\n",
    "Also, when re-scoring our problem, it makes more sense to update the current score with the alteration instead of re-calculating the score from scratch.\n",
    "The score presumes a valid problem state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3234972a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def score(problem_state: dict, endpoint_data_description: list, endpoint_cache_description: dict, request_description: dict) -> float:\n",
    "    score = 0\n",
    "    for (endpoint, video), request_number in request_description:\n",
    "        data_center_latency = endpoint_data_description[endpoint]\n",
    "        cache_latency = data_center_latency\n",
    "        for cache, videos in problem_state:\n",
    "            if video in videos:\n",
    "                cache_latency = min(cache_latency, endpoint_cache_description[(endpoint, cache)])\n",
    "        score += (data_center_latency - cache_latency) * request_number\n",
    "    return math.floor(score * 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f6a01b",
   "metadata": {},
   "source": [
    "### Updating the score\n",
    "Now, for computational efficiency effects, we create a re-score function that based on a cache change, a current score and the descriptions, updates the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cdafcc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def re_score(problem_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0d66925-7894-496c-b2de-2746b60a26a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Meta Heuristics\n",
    "\n",
    "### Tabu \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef62bc26-790f-4a89-a414-b3dec94dd923",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m     best \u001b[38;5;241m=\u001b[39m initial_solution\n\u001b[1;32m     17\u001b[0m     best_score \u001b[38;5;241m=\u001b[39m score(initial_solution,endpoint_data_description,endpoint_cache_description,request_description)\n\u001b[0;32m---> 18\u001b[0m \u001b[43mtabu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43mendpoint_data_description\u001b[49m\u001b[43m,\u001b[49m\u001b[43mendpoint_cache_description\u001b[49m\u001b[43m,\u001b[49m\u001b[43mrequest_description\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 17\u001b[0m, in \u001b[0;36mtabu\u001b[0;34m(initial_solution, endpoint_data_description, endpoint_cache_description, request_description)\u001b[0m\n\u001b[1;32m     15\u001b[0m taboos \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;66;03m# List with the index of the dict\u001b[39;00m\n\u001b[1;32m     16\u001b[0m best \u001b[38;5;241m=\u001b[39m initial_solution\n\u001b[0;32m---> 17\u001b[0m best_score \u001b[38;5;241m=\u001b[39m \u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_solution\u001b[49m\u001b[43m,\u001b[49m\u001b[43mendpoint_data_description\u001b[49m\u001b[43m,\u001b[49m\u001b[43mendpoint_cache_description\u001b[49m\u001b[43m,\u001b[49m\u001b[43mrequest_description\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 4\u001b[0m, in \u001b[0;36mscore\u001b[0;34m(problem_state, endpoint_data_description, endpoint_cache_description, request_description)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscore\u001b[39m(problem_state: \u001b[38;5;28mdict\u001b[39m, endpoint_data_description: \u001b[38;5;28mlist\u001b[39m, endpoint_cache_description: \u001b[38;5;28mdict\u001b[39m, request_description: \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m      3\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (endpoint, video), request_number \u001b[38;5;129;01min\u001b[39;00m request_description:\n\u001b[1;32m      5\u001b[0m         data_center_latency \u001b[38;5;241m=\u001b[39m endpoint_data_description[endpoint]\n\u001b[1;32m      6\u001b[0m         cache_latency \u001b[38;5;241m=\u001b[39m data_center_latency\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "def tabu(initial_solution: dict,endpoint_data_description:list, endpoint_cache_description:dict,request_description:dict):\n",
    "    taboos = [] # List with the index of the dict\n",
    "    best = initial_solution\n",
    "    best_score = score(initial_solution,endpoint_data_description,endpoint_cache_description,request_description)\n",
    "tabu({},endpoint_data_description,endpoint_cache_description,request_description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec6b0bc-288b-4786-9d8f-b90b2a7ff325",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
